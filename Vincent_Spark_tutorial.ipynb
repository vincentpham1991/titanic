{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Author: Vincent Pham"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intro\n",
    "\n",
    "This is a tutorial on Spark using the Titanic data from kaggle. To get data, go to https://www.kaggle.com/c/titanic/data and download train.csv and test.csv.\n",
    "\n",
    "\n",
    "Next, to easily open up a Spark cluster, we will utilize AWS EMR (Note: Amazon charges per hour for the EMR). But, since this is a small dataset, you can also run this on your local computer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in the Data\n",
    "\n",
    "Now that we have the data, we need to read it into spark. We will run pyspark via the terminal for now. Locate the folder where you Installed Spark. my version is spark-1.5.2. Now that you are in this folder, enter <font color = 'teal'>./bin/pyspark</font> to open up PySpark. \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train_file = \"./train.csv\"\n",
    "#test_file = \"./test.csv\"\n",
    "\n",
    "train_file = \"/Users/vincentpham/Downloads/train.csv\"\n",
    "test_file = \"/Users/vincentpham/Downloads/test.csv\"\n",
    "\n",
    "train = sc.textFile(train_file)\n",
    "test = sc.textFile(test_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Creating the Map Function\n",
    "\n",
    "In PySpark, you can use Labeled points to denote your dependent variable and features. Before we run the model though, we need to clean the data. First, we need to remove the header from our dataset before running it through the map function. Another thing to consider is what features do we want to use? We will use passenger socio-economic class, sex, age, number of siblings/spouses on board, number of parents/children on board, and passenger fare. Our variable of interest is whether someone survived or not. We can now create our map function that runs through each line of the data and returns a LabelPoint object of survival indicator and an array of our features. This is done in parse_training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from numpy import array\n",
    "\n",
    "#remove header\n",
    "header_train = train.take(1)[0]\n",
    "header_test = test.take(1)[0]\n",
    "train = train.filter(lambda line: line != header_train)\n",
    "test = test.filter(lambda line: line != header_test)\n",
    "\n",
    "def parse_training(line):\n",
    "    line_split = line.split(\",\")\n",
    "    survived = line_split[1]\n",
    "    \n",
    "    ismale = 1.0\n",
    "    if line_split[5] != \"male\":\n",
    "        ismale = 0.0\n",
    "    pclass = float(line_split[2])\n",
    "    try:\n",
    "        age = float(line_split[6])\n",
    "    except:\n",
    "        age = -1.0\n",
    "    sib = float(line_split[7])\n",
    "    parch = float(line_split[8])\n",
    "    fare = float(line_split[10])\n",
    "    features = array([ismale, pclass, age,sib,parch,fare])\n",
    "    return LabeledPoint(survived, features)\n",
    "\n",
    "parsed_data = train.map(parse_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with Missing Data\n",
    "\n",
    "For age, there are some missing values. I chose to remove observations that are missing age, but other methods can be applied such as setting the age to the mean or median against the total. Another idea, is to set it against the mean against the prefix of the name (for example, if the person has a Dr. in the prefix, then take the mean across all Doctors with age and apply it to the age of Doctors without age), but I will not try it here. We will use the filter function to remove observations where we marked age as missing with -1.0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parsed_data = parsed_data.filter(lambda line: line.features[2] != -1.0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Validation Set\n",
    "\n",
    "Since the test file does not include a column for whether a passenger survived or not, we can not use it to test our model. Thus, we have to split our training dataset into a training set and a validation set. I will divide it by 70%/30% respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data, valid_data = parsed_data.randomSplit((.7,.3), seed = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "\n",
    "Spark has two algorithms to solve logistic regression: (1) mini-batch gradient descent and (2) L-BFGS. If you want a faster convergence, then L-BFGS is recommended, which we will be using in this tutorial. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS\n",
    "\n",
    "logit_model = LogisticRegressionWithLBFGS.train(train_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to measure the classification error on our validation data, we use map on the valid_data RDD and the model to predict each test point class. Classification results are returned in pairs, with the actual test label and the predicted one. This is used to calculate the classification error by using filter and count as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels_and_preds = valid_data.map(lambda p: (p.label, logit_model.predict(p.features)))\n",
    "test_accuracy = labels_and_preds.filter(lambda (v, p): v == p).count() / float(valid_data.count())\n",
    "print \"Test accuracy is %1.4f\" % round(test_accuracy,4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running a logistic regression with age, gender, class, siblings/spouse, children/parent, and ticket price gave a 79% accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative Model: \n",
    "\n",
    "There are many other combinations of features that we can test out. Using intuition, it makes more sense if we reduce the number of features to gender (since women are more likely to get onto life boats), class (rich people may have more influences on getting a way to safety), and age (children and older people may have preference). For age, we will create dummy variables for whether someone is a child (younger than 13 years old) and whether someone is an elder (older than 60 years olds). Below we will run this model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_training_reduced(line):\n",
    "    line_split = line.split(\",\")\n",
    "    survived = line_split[1]\n",
    "    \n",
    "    ismale = 1.0\n",
    "    if line_split[5] != \"male\":\n",
    "        ismale = 0.0\n",
    "    pclass = float(line_split[2])\n",
    "    \n",
    "    ischild = 0\n",
    "    iselder = 0\n",
    "    try:\n",
    "        age = float(line_split[6])\n",
    "        if age <= 13:\n",
    "            ischild = 1\n",
    "        elif age >= 60:\n",
    "            iselder = 60\n",
    "    except:\n",
    "        ischild = -1\n",
    "        iselder = -1\n",
    "    features = array([ismale, pclass, ischild, iselder])\n",
    "    return LabeledPoint(survived, features)\n",
    "\n",
    "parsed_data = train.map(parse_training_reduced)\n",
    "#Remove observations that are missing age\n",
    "parsed_data = parsed_data.filter(lambda line: line.features[2] != -1.0)\n",
    "train_data, valid_data = parsed_data.randomSplit((.7,.3), seed = 1)\n",
    "\n",
    "logit_model = LogisticRegressionWithLBFGS.train(train_data)\n",
    "\n",
    "labels_and_preds = valid_data.map(lambda p: (p.label, logit_model.predict(p.features)))\n",
    "test_accuracy = labels_and_preds.filter(lambda (v, p): v == p).count() / float(valid_data.count())\n",
    "print \"Test accuracy is %1.4f\" % round(test_accuracy,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy for the reduced model is 79.91%. This is not a huge increase in accuracy, but with a small sample size, and smaller feature set, it is something to consider."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submitting to Kaggle\n",
    "\n",
    "Afterall, this is a kaggle competition, so we need to export our prediction on the test data that Kaggle provides. I had to create a seperate map function since the number of columns are different between the test and train dataset due to the lack of survival indicator. To read it back into a csv file, I had to create another map function that concatonates the tuple of passenger id and survival prediction. Use the saveAsTextFile function to save the predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_testing(line):\n",
    "    line_split = line.split(\",\")\n",
    "    person = line_split[0]\n",
    "    ismale = 1.0\n",
    "    if line_split[4] != \"male\":\n",
    "        ismale = 0.0\n",
    "    pclass = float(line_split[1])\n",
    "    \n",
    "    ischild = 0\n",
    "    iselder = 0\n",
    "    try:\n",
    "        age = float(line_split[6])\n",
    "        if age <= 13:\n",
    "            ischild = 1\n",
    "        elif age >= 60:\n",
    "            iselder = 60\n",
    "    except:\n",
    "        pass\n",
    "    features = array([ismale, pclass, ischild, iselder])\n",
    "    return (person,features)\n",
    "\n",
    "test_data = test.map(parse_testing)\n",
    "\n",
    "def toCSVLine(data):\n",
    "    return ','.join(str(d) for d in data)\n",
    "\n",
    "preds = test_data.map(lambda p : (p[0],logit_model.predict(p[1])))\n",
    "\n",
    "header = [(\"PassengerId\",\"Survived\")]\n",
    "preds = sc.parallelize(header+preds.collect())\n",
    "lines = preds.map(toCSVLine)\n",
    "lines.repartition(1).saveAsTextFile('~./preds.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submitting this to Kaggle, I received 0.76555. Not great, but a good effort for my first attempt at Spark. "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
